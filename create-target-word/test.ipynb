{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.apikey = \"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\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m openai\u001b[39m.\u001b[39mapi_key_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./key.txt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdo you know shimada hajime？\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m completion \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m      5\u001b[0m \tmodel \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     messages \u001b[39m=\u001b[39;49m [\n\u001b[1;32m      7\u001b[0m \t\t{\n\u001b[1;32m      8\u001b[0m \t\t\t\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m : \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m : question,\n\u001b[1;32m     10\u001b[0m \t\t}\n\u001b[1;32m     11\u001b[0m \t]\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m response \u001b[39m=\u001b[39m completion\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/miniconda3/envs/env04/lib/python3.9/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/miniconda3/envs/env04/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/miniconda3/envs/env04/lib/python3.9/site-packages/openai/api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    217\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    218\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/miniconda3/envs/env04/lib/python3.9/site-packages/openai/api_requestor.py:620\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    613\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    614\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    615\u001b[0m         )\n\u001b[1;32m    616\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    617\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    619\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 620\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    621\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    622\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    623\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    624\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    625\u001b[0m         ),\n\u001b[1;32m    626\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    627\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/env04/lib/python3.9/site-packages/openai/api_requestor.py:683\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    681\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    682\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 683\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    684\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    685\u001b[0m     )\n\u001b[1;32m    686\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
     ]
    }
   ],
   "source": [
    "import openai\n",
    "openai.api_key_path = \"./key.txt\"\n",
    "question = \"do you know shimada hajime？\"\n",
    "completion = openai.ChatCompletion.create(\n",
    "\tmodel = \"gpt-3.5-turbo\",\n",
    "    messages = [\n",
    "\t\t{\n",
    "\t\t\t\"role\" : \"user\",\n",
    "            \"content\" : question,\n",
    "\t\t}\n",
    "\t]\n",
    ")\n",
    "response = completion.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 19:36:49 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d224a0106cbd4692b964af01cdd03dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 19:36:50 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| depparse  | combined |\n",
      "========================\n",
      "\n",
      "2023-04-27 19:36:50 INFO: Use device: gpu\n",
      "2023-04-27 19:36:50 INFO: Loading: tokenize\n",
      "2023-04-27 19:36:53 INFO: Loading: pos\n",
      "2023-04-27 19:36:54 INFO: Loading: lemma\n",
      "2023-04-27 19:36:54 INFO: Loading: depparse\n",
      "2023-04-27 19:36:54 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"he\",\n",
      "      \"lemma\": \"he\",\n",
      "      \"upos\": \"PRON\",\n",
      "      \"xpos\": \"PRP\",\n",
      "      \"feats\": \"Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs\",\n",
      "      \"head\": 2,\n",
      "      \"deprel\": \"nsubj\",\n",
      "      \"start_char\": 0,\n",
      "      \"end_char\": 2\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"text\": \"came\",\n",
      "      \"lemma\": \"come\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VBD\",\n",
      "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"start_char\": 3,\n",
      "      \"end_char\": 7\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"text\": \"to\",\n",
      "      \"lemma\": \"to\",\n",
      "      \"upos\": \"ADP\",\n",
      "      \"xpos\": \"IN\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"case\",\n",
      "      \"start_char\": 8,\n",
      "      \"end_char\": 10\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"text\": \"my\",\n",
      "      \"lemma\": \"my\",\n",
      "      \"upos\": \"PRON\",\n",
      "      \"xpos\": \"PRP$\",\n",
      "      \"feats\": \"Number=Sing|Person=1|Poss=Yes|PronType=Prs\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"nmod:poss\",\n",
      "      \"start_char\": 11,\n",
      "      \"end_char\": 13\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"text\": \"house\",\n",
      "      \"lemma\": \"house\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 2,\n",
      "      \"deprel\": \"obl\",\n",
      "      \"start_char\": 14,\n",
      "      \"end_char\": 19\n",
      "    }\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp = stanza.Pipeline(lang=\"en\", processors=\"tokenize,pos,lemma,depparse\")\n",
    "doc = nlp('he came to my house')\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('dont try')\n",
    "print(doc.sentences[0].words[1].pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A closely related task to AES is classifying texts into L2 proficiency levels which consists of predicting at which language learning stage a text can be produced or understood by a L2 learner, rather than assigning a grade within a pass-fail range. The CEFR, the scale of proficiency levels adopted in our experiments, contains guidelines for the standardization of language teaching and assessment across languages and countries (Council of Europe, 2001). It provides a common metalanguage to talk about objectives, assessment, (Little, 2011), and it defines language competences at six proficiency levels (A1, A2, B1, B2, C1, C2) where A1 is the beginner level. Since the publication of the CEFR guidelines in 2001, several countries have adopted the system, but its practical application has proven to be rather non-straightforward since the descriptions of the competences at each level remain vague (Little, 2011; North, 2007). The past few years have seen an increasing interest in the CEFR-level classification of both L2 input and output texts. In the case of coursebook texts such a classification has also been referred to as L2 readability and it has been investigated for, among others, French (Franc¸ois and Fairon, 2012), Portuguese (Branco et al., 2014), Chinese (Sung et al., 2015), Swedish (Pilan et al., 2015), and English (Xia et al., ´ 2016). Apart from L2 input texts, CEFR-level annotated L2 learner corpora are also available for a number of languages including but not limited to English (Nicholls, 2003), Estonian (Vajjala and Loo, 2014) ˜ and German (Hancke and Meurers, 2013). Moreover, MERLIN (Wisniewski et al., 2013) is a trilingual learner corpus comprised of written productions of L2 learners of Czech, German, and Italian also linked to CEFR levels. Despite the availability of annotated corpora for several languages, the number of projects targeting the automatic CEFR-level classification of learner essays has remained rather limited. Previously reported results for this task in terms of accuracy include 61% for German (Hancke and Meurers, 2013) and 79% for Estonian (Vajjala and Loo, 2014). \n"
     ]
    }
   ],
   "source": [
    "a = input()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['back', 'book', 'call', 'control', 'hate', 'love', 'post', 'guess', 'notice', 'win', 'finish', 'search', 'throw', 'try', 'walk', 'approach', 'blame', 'block', 'burn', 'challenge', 'claim', 'concern', 'design', 'display', 'drink', 'escape', 'experience', 'fight', 'force', 'form', 'guide', 'help', 'hurt', 'increase', 'issue', 'kick', 'look', 'permit', 'play', 'question', 'record', 'rent', 'repeat', 'report', 'respect', 'review', 'surprise', 'touch', 'trust', 'turn', 'wash', 'welcome', 'get', 'allow', 'buy', 'give', 'know', 'learn', 'like', 'mean', 'need', 'read', 'see', 'show', 'tell', 'want', 'carry', 'choose', 'feel', 'kill', 'open', 'pull', 'reach', 'sell', 'stop', 'think', 'work', 'write', 'attend', 'beat', 'cost', 'draw', 'drop', 'end', 'fill', 'move', 'own', 'pass', 'plan', 'recommend', 'seek', 'test', 'treat', 'assist', 'clean', 'combine', 'come', 'cook', 'count', 'destroy', 'file', 'grow', 'lack', 'match', 'name', 'point', 'roll', 'shoot', 'sign', 'speak', 'suit', 'talk', 'wish', 'lock', 'value', 'aid', 'blow', 'cast', 'last', 'link', 'mind', 'paint', 'recall', 'sing', 'stand', 'stay', 'train', 'weigh', 'wrap', 'damage', 'interview', 'note', 'risk', 'shape', 'beg', 'fail', 'hang', 'judge', 'load', 'predict', 'seem', 'slow', 'state', 'stick', 'tie', 'travel', 'wait', 'warn', 'attempt', 'exchange', 'head', 'land', 'market', 'sum', 'type', 'cite', 'light', 'rule', 'smell', 'top', 'heat', 'reward', 'tap', 'command', 'fly', 'house', 'mail', 'taste', 'base', 'effect', 'estimate', 'jump', 'rate', 'rest', 'ship', 'sound', 'step', 'suspect', 'vote', 'calm', 'can', 'entertain', 'hunt', 'rock', 'sit', 'wave', 'pound', 'fine', 'succeed', 'strain', 'bat', 'prop', 'pen', 'lease', 'smile', 'dance', 'coat', 'raid', 'style', 'bag', 'wage', 'net', 'pit', 'coin', 'root', 'ground', 'trail', 'cash', 'group', 'lean', 'boot', 'shame', 'stain', 'fish', 'pile', 'race', 'rise', 'time', 'rain', 'stem', 'swim', 'crowd']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('/home/kotaro/work/metaphor-analysis-m1/data/target-word/target-words.csv')\n",
    "df2 = pd.read_csv('/home/kotaro/work/metaphor-analysis-m1/data/target-word/verb-classed.csv')\n",
    "target_verb_list = list(df1.iloc[:,0])\n",
    "verb_list = list(df2.iloc[:,0])\n",
    "for verb in verb_list:\n",
    "    if verb in target_verb_list:\n",
    "        verb_list.remove(verb)\n",
    "print(verb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representative name for the entity: ところ\n",
      "Entity type: OTHER\n",
      "Salience score: 1.0\n",
      "Mention text: ところ\n",
      "Mention type: COMMON\n",
      "Representative name for the entity: 一\n",
      "Entity type: NUMBER\n",
      "Salience score: 0.0\n",
      "value: 1\n",
      "Mention text: 一\n",
      "Mention type: TYPE_UNKNOWN\n",
      "Language of the text: ja\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import language_v1\n",
    "import os\n",
    "\n",
    "#下記はサンプルなので、ご自分のキーファイルの場所に置き換えてください。\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/home/kotaro/work/metaphor-analysis-m1/key/metaphor-detection-388310-0b6e16511d43.json\"\n",
    "\n",
    "def sample_analyze_entities(text_content):\n",
    "    \"\"\"\n",
    "    Analyzing Entities in a String\n",
    "\n",
    "    Args:\n",
    "      text_content The text content to analyze\n",
    "    \"\"\"\n",
    "\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "    type_ = language_v1.Document.Type.PLAIN_TEXT\n",
    "\n",
    "    # サンプルではen ですが、日本語でjaに変更します。\n",
    "    language = \"ja\"\n",
    "    \n",
    "    document = {\"content\": text_content, \"type_\": type_, \"language\": language}\n",
    "\n",
    "    # Available values: NONE, UTF8, UTF16, UTF32\n",
    "    encoding_type = language_v1.EncodingType.UTF8\n",
    "\n",
    "    response = client.analyze_entities(request = {'document': document, 'encoding_type': encoding_type})\n",
    "\n",
    "    # Loop through entitites returned from the API\n",
    "    for entity in response.entities:\n",
    "        print(u\"Representative name for the entity: {}\".format(entity.name))\n",
    "\n",
    "        print(u\"Entity type: {}\".format(language_v1.Entity.Type(entity.type_).name))\n",
    "\n",
    "        print(u\"Salience score: {}\".format(entity.salience))\n",
    "\n",
    "        # Loop over the metadata associated with entity. For many known entities,\n",
    "        # the metadata is a Wikipedia URL (wikipedia_url) and Knowledge Graph MID (mid).\n",
    "        # Some entity types may have additional metadata, e.g. ADDRESS entities\n",
    "        # may have metadata for the address street_name, postal_code, et al.\n",
    "        for metadata_name, metadata_value in entity.metadata.items():\n",
    "            print(u\"{}: {}\".format(metadata_name, metadata_value))\n",
    "\n",
    "        # Loop over the mentions of this entity in the input document.\n",
    "        # The API currently supports proper noun mentions.\n",
    "        for mention in entity.mentions:\n",
    "            print(u\"Mention text: {}\".format(mention.text.content))\n",
    "\n",
    "            # Get the mention type, e.g. PROPER for proper noun\n",
    "            print(\n",
    "                u\"Mention type: {}\".format(language_v1.EntityMention.Type(mention.type_).name)\n",
    "            )\n",
    "\n",
    "    # Get the language of the text, which will be the same as\n",
    "    # the language specified in the request or, if not specified,\n",
    "    # the automatically-detected language.\n",
    "    print(u\"Language of the text: {}\".format(response.language))\n",
    "\n",
    "sample_analyze_entities('昔々、あるところに一匹のこざるがおりました。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import language_v1\n",
    "\n",
    "def sample_analyze_sentiment(content):\n",
    "\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "\n",
    "    # content = 'Your text to analyze, e.g. Hello, world!'\n",
    "\n",
    "    if isinstance(content, bytes):\n",
    "        content = content.decode(\"utf-8\")\n",
    "\n",
    "    type_ = language_v1.Document.Type.PLAIN_TEXT\n",
    "    document = {\"type_\": type_, \"content\": content}\n",
    "\n",
    "    response = client.analyze_sentiment(request={\"document\": document})\n",
    "    sentiment = response.document_sentiment\n",
    "    print(f\"Score: {sentiment.score}\")\n",
    "    print(f\"Magnitude: {sentiment.magnitude}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad argument type for built-in operation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/env04/lib/python3.9/site-packages/proto/marshal/rules/message.py:36\u001b[0m, in \u001b[0;36mMessageRule.to_proto\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[39m# Try the fast path first.\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_descriptor(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mvalue)\n\u001b[1;32m     37\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m     38\u001b[0m     \u001b[39m# If we have a type error,\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[39m# try the slow path in case the error\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[39m# was an int64/string issue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: bad argument type for built-in operation",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample_analyze_sentiment([\u001b[39m'\u001b[39;49m\u001b[39mI teach English to my classmate.\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mhello,world\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "Cell \u001b[0;32mIn [1], line 15\u001b[0m, in \u001b[0;36msample_analyze_sentiment\u001b[0;34m(content)\u001b[0m\n\u001b[1;32m     12\u001b[0m type_ \u001b[39m=\u001b[39m language_v1\u001b[39m.\u001b[39mDocument\u001b[39m.\u001b[39mType\u001b[39m.\u001b[39mPLAIN_TEXT\n\u001b[1;32m     13\u001b[0m document \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtype_\u001b[39m\u001b[39m\"\u001b[39m: type_, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: content}\n\u001b[0;32m---> 15\u001b[0m response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49manalyze_sentiment(request\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mdocument\u001b[39;49m\u001b[39m\"\u001b[39;49m: document})\n\u001b[1;32m     16\u001b[0m sentiment \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mdocument_sentiment\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mScore: \u001b[39m\u001b[39m{\u001b[39;00msentiment\u001b[39m.\u001b[39mscore\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/env04/lib/python3.9/site-packages/google/cloud/language_v1/services/language_service/client.py:509\u001b[0m, in \u001b[0;36mLanguageServiceClient.analyze_sentiment\u001b[0;34m(self, request, document, encoding_type, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[39m# Minor optimization to avoid making a copy if the user passes\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[39m# in a language_service.AnalyzeSentimentRequest.\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[39m# There's no risk of modifying the input as we've already verified\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[39m# there are no flattened fields.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(request, language_service\u001b[39m.\u001b[39mAnalyzeSentimentRequest):\n\u001b[0;32m--> 509\u001b[0m     request \u001b[39m=\u001b[39m language_service\u001b[39m.\u001b[39;49mAnalyzeSentimentRequest(request)\n\u001b[1;32m    510\u001b[0m     \u001b[39m# If we have keyword arguments corresponding to fields on the\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[39m# request, apply these.\u001b[39;00m\n\u001b[1;32m    512\u001b[0m     \u001b[39mif\u001b[39;00m document \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/env04/lib/python3.9/site-packages/proto/message.py:570\u001b[0m, in \u001b[0;36mMessage.__init__\u001b[0;34m(self, mapping, ignore_unknown_fields, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    566\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnknown field for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, key)\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    569\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 570\u001b[0m     pb_value \u001b[39m=\u001b[39m marshal\u001b[39m.\u001b[39;49mto_proto(pb_type, value)\n\u001b[1;32m    571\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     \u001b[39m# Underscores may be appended to field names\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     \u001b[39m# that collide with python or proto-plus keywords.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[39m# See related issue\u001b[39;00m\n\u001b[1;32m    578\u001b[0m     \u001b[39m# https://github.com/googleapis/python-api-core/issues/227\u001b[39;00m\n\u001b[1;32m    579\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/env04/lib/python3.9/site-packages/proto/marshal/marshal.py:217\u001b[0m, in \u001b[0;36mBaseMarshal.to_proto\u001b[0;34m(self, proto_type, value, strict)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39m# Convert ordinary values.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m rule \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rules\u001b[39m.\u001b[39mget(proto_type, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_noop)\n\u001b[0;32m--> 217\u001b[0m pb_value \u001b[39m=\u001b[39m rule\u001b[39m.\u001b[39;49mto_proto(value)\n\u001b[1;32m    219\u001b[0m \u001b[39m# Sanity check: If we are in strict mode, did we get the value we want?\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39mif\u001b[39;00m strict \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(pb_value, proto_type):\n",
      "File \u001b[0;32m~/miniconda3/envs/env04/lib/python3.9/site-packages/proto/marshal/rules/message.py:41\u001b[0m, in \u001b[0;36mMessageRule.to_proto\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_descriptor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mvalue)\n\u001b[1;32m     37\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m     38\u001b[0m         \u001b[39m# If we have a type error,\u001b[39;00m\n\u001b[1;32m     39\u001b[0m         \u001b[39m# try the slow path in case the error\u001b[39;00m\n\u001b[1;32m     40\u001b[0m         \u001b[39m# was an int64/string issue\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrapper(value)\u001b[39m.\u001b[39m_pb\n\u001b[1;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m~/miniconda3/envs/env04/lib/python3.9/site-packages/proto/message.py:604\u001b[0m, in \u001b[0;36mMessage.__init__\u001b[0;34m(self, mapping, ignore_unknown_fields, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m         params[key] \u001b[39m=\u001b[39m pb_value\n\u001b[1;32m    603\u001b[0m \u001b[39m# Create the internal protocol buffer.\u001b[39;00m\n\u001b[0;32m--> 604\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m_pb\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_meta\u001b[39m.\u001b[39;49mpb(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams))\n",
      "\u001b[0;31mTypeError\u001b[0m: bad argument type for built-in operation"
     ]
    }
   ],
   "source": [
    "sample_analyze_sentiment(['I teach English to my classmate.','hello,world'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(movie_review_filename):\n",
    "    \"\"\"Run a sentiment analysis request on text within a passed filename.\"\"\"\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "\n",
    "    with open(movie_review_filename) as review_file:\n",
    "        # Instantiates a plain text document.\n",
    "        content = review_file.read()\n",
    "\n",
    "    document = language_v1.Document(\n",
    "        content=content, type_=language_v1.Document.Type.PLAIN_TEXT\n",
    "    )\n",
    "    annotations = client.analyze_sentiment(request={\"document\": document})\n",
    "\n",
    "    # Print the results\n",
    "    print_result(annotations)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('env04')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b7bc24eb23ff8ba1c328234605088829598a2c850927532c3c105401123e54e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
