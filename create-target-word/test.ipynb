{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.apikey = \"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\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m openai\u001b[39m.\u001b[39mapi_key_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./key.txt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdo you know shimada hajime？\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m completion \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m      5\u001b[0m \tmodel \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     messages \u001b[39m=\u001b[39;49m [\n\u001b[1;32m      7\u001b[0m \t\t{\n\u001b[1;32m      8\u001b[0m \t\t\t\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m : \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m : question,\n\u001b[1;32m     10\u001b[0m \t\t}\n\u001b[1;32m     11\u001b[0m \t]\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m response \u001b[39m=\u001b[39m completion\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/miniconda3/envs/env04/lib/python3.9/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/miniconda3/envs/env04/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/miniconda3/envs/env04/lib/python3.9/site-packages/openai/api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    217\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    218\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/miniconda3/envs/env04/lib/python3.9/site-packages/openai/api_requestor.py:620\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    613\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    614\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    615\u001b[0m         )\n\u001b[1;32m    616\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    617\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    619\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 620\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    621\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    622\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    623\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    624\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    625\u001b[0m         ),\n\u001b[1;32m    626\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    627\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/env04/lib/python3.9/site-packages/openai/api_requestor.py:683\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    681\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    682\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 683\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    684\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    685\u001b[0m     )\n\u001b[1;32m    686\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
     ]
    }
   ],
   "source": [
    "import openai\n",
    "openai.api_key_path = \"./key.txt\"\n",
    "question = \"do you know shimada hajime？\"\n",
    "completion = openai.ChatCompletion.create(\n",
    "\tmodel = \"gpt-3.5-turbo\",\n",
    "    messages = [\n",
    "\t\t{\n",
    "\t\t\t\"role\" : \"user\",\n",
    "            \"content\" : question,\n",
    "\t\t}\n",
    "\t]\n",
    ")\n",
    "response = completion.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 19:36:49 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d224a0106cbd4692b964af01cdd03dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 19:36:50 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| depparse  | combined |\n",
      "========================\n",
      "\n",
      "2023-04-27 19:36:50 INFO: Use device: gpu\n",
      "2023-04-27 19:36:50 INFO: Loading: tokenize\n",
      "2023-04-27 19:36:53 INFO: Loading: pos\n",
      "2023-04-27 19:36:54 INFO: Loading: lemma\n",
      "2023-04-27 19:36:54 INFO: Loading: depparse\n",
      "2023-04-27 19:36:54 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"he\",\n",
      "      \"lemma\": \"he\",\n",
      "      \"upos\": \"PRON\",\n",
      "      \"xpos\": \"PRP\",\n",
      "      \"feats\": \"Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs\",\n",
      "      \"head\": 2,\n",
      "      \"deprel\": \"nsubj\",\n",
      "      \"start_char\": 0,\n",
      "      \"end_char\": 2\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"text\": \"came\",\n",
      "      \"lemma\": \"come\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VBD\",\n",
      "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"start_char\": 3,\n",
      "      \"end_char\": 7\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"text\": \"to\",\n",
      "      \"lemma\": \"to\",\n",
      "      \"upos\": \"ADP\",\n",
      "      \"xpos\": \"IN\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"case\",\n",
      "      \"start_char\": 8,\n",
      "      \"end_char\": 10\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"text\": \"my\",\n",
      "      \"lemma\": \"my\",\n",
      "      \"upos\": \"PRON\",\n",
      "      \"xpos\": \"PRP$\",\n",
      "      \"feats\": \"Number=Sing|Person=1|Poss=Yes|PronType=Prs\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"nmod:poss\",\n",
      "      \"start_char\": 11,\n",
      "      \"end_char\": 13\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"text\": \"house\",\n",
      "      \"lemma\": \"house\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 2,\n",
      "      \"deprel\": \"obl\",\n",
      "      \"start_char\": 14,\n",
      "      \"end_char\": 19\n",
      "    }\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp = stanza.Pipeline(lang=\"en\", processors=\"tokenize,pos,lemma,depparse\")\n",
    "doc = nlp('he came to my house')\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('dont try')\n",
    "print(doc.sentences[0].words[1].pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a semantic study which attempts to explore how Taiwanese process time metaphors. According to Shuell (1990: 102), “If a picture is worth 1,000 words, a metaphor is worth 1,000 pictures!” By breaking literal meanings, metaphors create thousands of possibilities. The way we structure the thousand pictures relies on conceptual metaphor. Conceptual metaphor is people’s underlying cognitive level as the bridge between language and thought. By assimilating the two different domains, conceptual metaphor specifies the concrete idea into abstract entity. In general, conceptual metaphor is the surface structures which make metaphors understandable. (Lakoff and Johnnson, 1980; Goddard, 1998; McGlone, 2007; Charteris-Black & Ennis, 2001). As Lakoff (1993:228) claims, “We do not have detectors for time. Thus, it makes good biological sense that time should be understood in terms of things and motion.” That is, the comprehension of the abstract time understood via space is biologically determined. The two space → time metaphors under examination are time-moving and ego-moving metaphors. Based on the study of Gentner, Imai, and Boroditsky (2002) in which English native speakers conceptualize ego-moving metaphor faster, two research goals are proposed: Chinese native speakers and EFL learners process ego-moving metaphors better. In order to answer the two research questions, this paper is organized as follows, (1) Introduction, (2) the theoretical framework on temporal metaphors, (3) the methodology, (4) results, (5) discussion, and (6) conclusions. \n"
     ]
    }
   ],
   "source": [
    "a = input()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['back', 'book', 'call', 'control', 'hate', 'love', 'post', 'guess', 'notice', 'win', 'finish', 'search', 'throw', 'try', 'walk', 'approach', 'blame', 'block', 'burn', 'challenge', 'claim', 'concern', 'design', 'display', 'drink', 'escape', 'experience', 'fight', 'force', 'form', 'guide', 'help', 'hurt', 'increase', 'issue', 'kick', 'look', 'permit', 'play', 'question', 'record', 'rent', 'repeat', 'report', 'respect', 'review', 'surprise', 'touch', 'trust', 'turn', 'wash', 'welcome', 'get', 'allow', 'buy', 'give', 'know', 'learn', 'like', 'mean', 'need', 'read', 'see', 'show', 'tell', 'want', 'carry', 'choose', 'feel', 'kill', 'open', 'pull', 'reach', 'sell', 'stop', 'think', 'work', 'write', 'attend', 'beat', 'cost', 'draw', 'drop', 'end', 'fill', 'move', 'own', 'pass', 'plan', 'recommend', 'seek', 'test', 'treat', 'assist', 'clean', 'combine', 'come', 'cook', 'count', 'destroy', 'file', 'grow', 'lack', 'match', 'name', 'point', 'roll', 'shoot', 'sign', 'speak', 'suit', 'talk', 'wish', 'lock', 'value', 'aid', 'blow', 'cast', 'last', 'link', 'mind', 'paint', 'recall', 'sing', 'stand', 'stay', 'train', 'weigh', 'wrap', 'damage', 'interview', 'note', 'risk', 'shape', 'beg', 'fail', 'hang', 'judge', 'load', 'predict', 'seem', 'slow', 'state', 'stick', 'tie', 'travel', 'wait', 'warn', 'attempt', 'exchange', 'head', 'land', 'market', 'sum', 'type', 'cite', 'light', 'rule', 'smell', 'top', 'heat', 'reward', 'tap', 'command', 'fly', 'house', 'mail', 'taste', 'base', 'effect', 'estimate', 'jump', 'rate', 'rest', 'ship', 'sound', 'step', 'suspect', 'vote', 'calm', 'can', 'entertain', 'hunt', 'rock', 'sit', 'wave', 'pound', 'fine', 'succeed', 'strain', 'bat', 'prop', 'pen', 'lease', 'smile', 'dance', 'coat', 'raid', 'style', 'bag', 'wage', 'net', 'pit', 'coin', 'root', 'ground', 'trail', 'cash', 'group', 'lean', 'boot', 'shame', 'stain', 'fish', 'pile', 'race', 'rise', 'time', 'rain', 'stem', 'swim', 'crowd']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('/home/kotaro/work/metaphor-analysis-m1/data/target-word/target-words.csv')\n",
    "df2 = pd.read_csv('/home/kotaro/work/metaphor-analysis-m1/data/target-word/verb-classed.csv')\n",
    "target_verb_list = list(df1.iloc[:,0])\n",
    "verb_list = list(df2.iloc[:,0])\n",
    "for verb in verb_list:\n",
    "    if verb in target_verb_list:\n",
    "        verb_list.remove(verb)\n",
    "print(verb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('env04')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b7bc24eb23ff8ba1c328234605088829598a2c850927532c3c105401123e54e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
